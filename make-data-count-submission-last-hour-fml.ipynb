{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":260941893,"sourceType":"kernelVersion"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\n\ntqdm.pandas()\n\nMODEL_NAME = \"/kaggle/working/all-MiniLM-L6-v2\"\nK = 5\nID_LABELS = [\"dataset_id\", \"article_id\", \"id\", \"DOI\", \"url\"]\nTRAIN_LABELS = [\n    'title', 'segments', 'extension', 'abstract', 'publisher',\n    'copyright', 'issued_year', 'all_authors', 'categories'\n]\nALL_FIELDS = [\n    'article_id','text','extension','source','dataset_id','dataset_id_cited','type',\n    'id','categories','abstract','DOI','publisher','title','URL','copyright',\n    'issued_year','all_authors','y'\n]\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass Basement(SentenceTransformer):\n    def __init__(self, initial_data: pd.DataFrame):\n        super().__init__(MODEL_NAME, device=DEVICE)\n        self.x = self.preprocess_data(initial_data)\n        self.to(DEVICE)\n        self.eval()\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def _meta_conditioner(self, row):\n        combined_prompt = f\"\"\"\n        Title: {row.get('title', '')}\n        Abstract: {row.get('abstract', '')}\n        Author: {row.get('all_authors', '')}\n        Extension File Type: {row.get('extension', '')}\n        Publisher: {row.get('publisher', '')}\n        Category: {row.get('categories', '')}\n        Issued Year: {row.get('issued_year', '')}\n        Copyright: {row.get('copyright', '')}\n        \"\"\"\n        meta_embedding = self.encode(\n            [combined_prompt],\n            convert_to_numpy=True,\n            output_value=\"sentence_embedding\",\n            show_progress_bar=False\n        )\n        text_embedding = row.get('segments', None)\n\n        if isinstance(text_embedding, np.ndarray):\n            if text_embedding.ndim == 1:\n                text_embedding = np.expand_dims(text_embedding, axis=0)\n            return np.vstack((meta_embedding, text_embedding))\n\n        return meta_embedding  # fallback to meta only\n\n    def _segment_text(self, s: str, k: int = K) -> np.ndarray:\n        if not isinstance(s, str) or not s:\n            return np.zeros((k, self.get_sentence_embedding_dimension()), dtype=np.float32)\n        n = len(s)\n        idx = np.linspace(0, n, k + 1, dtype=int)\n        chunks = [s[idx[i]:idx[i+1]] for i in range(k)]\n        emb = self.encode(chunks, convert_to_numpy=True, show_progress_bar=False)\n        return np.asarray(emb, dtype=np.float32).reshape(k, -1)\n\n    def preprocess_data(\n        self,\n        data: pd.DataFrame,\n        k: int = K,\n        train_labels: list[str] | None = None\n    ) -> pd.DataFrame:\n        train_labels = train_labels or TRAIN_LABELS\n        df = data.copy()\n\n        # 1) create 'segments'\n        df[\"segments\"] = df[\"text\"].apply(lambda x: self._segment_text(x, k))\n\n        # 2) explode multi-valued columns\n        for col in (\"all_authors\", \"segments\", \"categories\"):\n            if col in df.columns:\n                df = df.explode(col, ignore_index=True)\n\n        # 3) drop duplicated rows\n        subset_cols = [c for c in train_labels if c in df.columns and c != \"segments\"]\n        if subset_cols:\n            df = df.drop_duplicates(subset=subset_cols, keep=\"first\")\n\n        # 4) fills / cleaning\n        if \"issued_year\" in df.columns:\n            mode_series = df[\"issued_year\"].dropna().mode()\n            if not mode_series.empty:\n                df[\"issued_year\"] = df[\"issued_year\"].fillna(mode_series.iloc[0])\n\n        if \"copyright\" in df.columns:\n            df[\"copyright\"] = (\n                df[\"copyright\"].astype(\"string\").str.strip().replace({\"\": \"Unknown\"})\n            )\n\n        if \"categories\" in df.columns:\n            df[\"categories\"] = df[\"categories\"].astype(\"string\").fillna(\"Unknown\")\n\n        df[\"inputs\"] = df.apply(self._meta_conditioner, axis=1)\n        return df\n\n    def predict(self, inputs: dict, threshold: float = 0.5):\n        text = inputs.get(\"text\")\n        if not text:\n            return None\n\n        # Query embedding\n        segment = self._segment_text(text)\n        inputs = dict(inputs, segments=segment)\n        input_emb = self._meta_conditioner(inputs)\n        if input_emb.ndim == 1:\n            input_emb = input_emb.reshape(1, -1)\n        q = input_emb.mean(axis=0, keepdims=True)\n\n        q_tensor = torch.tensor(q, device=DEVICE, dtype=torch.float32)\n\n        max_pred = None\n        for grp, row in self.x.groupby(['article_id', 'dataset_id', 'source']):\n            valid_inputs = [\n                x for x in row['inputs'].values\n                if isinstance(x, np.ndarray) and x.size > 0\n            ]\n            if not valid_inputs:\n                continue\n\n            train_sample = np.vstack(valid_inputs)\n            train_tensor = torch.tensor(train_sample, device=DEVICE, dtype=torch.float32)\n\n            avg = float(cos_sim(train_tensor, q_tensor).mean())\n            prev = max_pred['score'] if max_pred else None\n            if (max_pred is None) or (avg > prev):\n                source_type = grp[2] if avg >= threshold else \"Unknown\"\n                max_pred = {\n                    'article_id': grp[0],\n                    'dataset_id': grp[1],\n                    'type': source_type,\n                    'score': avg,\n                }\n        return max_pred\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# Run pipeline\n# ======================\ntrain_data = pd.read_parquet('/kaggle/input/make-data-count-data-preparation/train_dataset.parquet')\ntest_data = pd.read_parquet('/kaggle/input/make-data-count-data-preparation/test_dataset.parquet')\n\nbase = Basement(train_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction = []\n\nfor _, row in test_data.iterrows():\n    pred = base.predict(row.to_dict())\n    if pred:\n        prediction.append(pred)\n\nsubmission = pd.DataFrame.from_records(prediction)\nsubmission = submission.sort_values([\"article_id\", \"dataset_id\", \"type\"]).reset_index(drop=True)\nsubmission['row_index'] = submission.index\n\nsubmission[['row_index', 'article_id', 'dataset_id', 'type']].to_csv('/kaggle/working/submission.csv', index=False)\nprint('data submitted!', submission.head(5))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}