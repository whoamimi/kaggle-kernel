{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"sourceType":"competition"},{"sourceId":260941893,"sourceType":"kernelVersion"}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Download requirements for this notebook \n!pip install --upgrade pip\n!pip install -q ipywidgets\n!pip install \"transformers<4.44\" \"sentence-transformers>=2.2.2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T23:34:37.882939Z","iopub.execute_input":"2025-09-09T23:34:37.883306Z","iopub.status.idle":"2025-09-09T23:35:59.490718Z","shell.execute_reply.started":"2025-09-09T23:34:37.883278Z","shell.execute_reply":"2025-09-09T23:35:59.486107Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (25.2)\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79d596d3a950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pip/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79d596d3ac50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pip/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79d596d3afb0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/pip/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m^C\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip\", line 5, in <module>\n    from pip._internal.cli.main import main\n  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/main.py\", line 11, in <module>\n    from pip._internal.cli.autocompletion import autocomplete\n  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/autocompletion.py\", line 12, in <module>\n    from pip._internal.cli.main_parser import create_main_parser\n  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n    from pip._internal.build_env import get_runnable_pip\n  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/build_env.py\", line 19, in <module>\n    from pip._internal.cli.spinners import open_spinner\n  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/cli/spinners.py\", line 22, in <module>\n    from pip._internal.utils.logging import get_console, get_indentation\n  File \"/usr/local/lib/python3.10/site-packages/pip/_internal/utils/logging.py\", line 25, in <module>\n    from pip._vendor.rich.logging import RichHandler\n  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/rich/logging.py\", line 15, in <module>\n    from .traceback import Traceback\n  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/rich/traceback.py\", line 23, in <module>\n    from pip._vendor.pygments.lexers import guess_lexer_for_filename\n  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pygments/lexers/__init__.py\", line 19, in <module>\n    from pip._vendor.pygments.plugin import find_plugin_lexers\n  File \"/usr/local/lib/python3.10/site-packages/pip/_vendor/pygments/plugin.py\", line 35, in <module>\n    from importlib.metadata import entry_points\n  File \"/usr/local/lib/python3.10/importlib/metadata/__init__.py\", line 21, in <module>\n    from ._itertools import unique_everseen\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n  File \"<frozen importlib._bootstrap_external>\", line 1439, in find_spec\n  File \"<frozen importlib._bootstrap_external>\", line 1411, in _get_spec\n  File \"<frozen importlib._bootstrap_external>\", line 1544, in find_spec\n  File \"<frozen importlib._bootstrap_external>\", line 147, in _path_stat\nKeyboardInterrupt\nRequirement already satisfied: transformers<4.44 in /usr/local/lib/python3.10/site-packages (4.43.4)\nRequirement already satisfied: sentence-transformers>=2.2.2 in /usr/local/lib/python3.10/site-packages (5.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (0.33.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (2.32.4)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (0.6.0rc0)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers<4.44) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers<4.44) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers<4.44) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers<4.44) (1.1.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/site-packages (from sentence-transformers>=2.2.2) (2.6.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from sentence-transformers>=2.2.2) (1.7.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from sentence-transformers>=2.2.2) (1.15.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/site-packages (from sentence-transformers>=2.2.2) (11.3.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.2) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.2) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers<4.44) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers<4.44) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers<4.44) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers<4.44) (2025.6.15)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.2.2) (1.5.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.2.2) (3.6.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\nos.environ[\"HF_HUB_OFFLINE\"] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# 2) Install deps (Kaggle usually has these, but safe to ensure)\n!pip -q install sentence-transformers huggingface_hub\n\nimport shutil, json, glob, torch\nfrom huggingface_hub import snapshot_download\nfrom sentence_transformers import SentenceTransformer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n\n# 3) Start fresh: remove any broken local copy\nlocal_dir = \"./all-MiniLM-L6-v2\"\nshutil.rmtree(local_dir, ignore_errors=True)\n\n# 4) Download the FULL repo to a portable folder (no symlinks!)\nsnap_path = snapshot_download(\n    repo_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    local_dir=local_dir,\n    local_dir_use_symlinks=False,   # <-- real files, not symlinks\n    revision=\"main\",                 # pin a branch/commit if you like\n)\n\nprint(\"Snapshot downloaded to:\", snap_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T23:35:59.691219Z","iopub.execute_input":"2025-09-09T23:35:59.691455Z","iopub.status.idle":"2025-09-09T23:36:04.611251Z","shell.execute_reply.started":"2025-09-09T23:35:59.691428Z","shell.execute_reply":"2025-09-09T23:36:04.606997Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0mUsing device: cpu\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\nFetching 30 files: 100%|██████████| 30/30 [00:02<00:00, 10.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Snapshot downloaded to: /kaggle/working/all-MiniLM-L6-v2\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# --- 1) Get TPU device\nimport torch\nimport torch_xla.core.xla_model as xm\n\ndevice = xm.xla_device()  # e.g. xla:1 on v3-8\nprint(\"TPU device:\", device)\nTARGET_DEVICE = device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T23:39:59.607992Z","iopub.execute_input":"2025-09-09T23:39:59.608337Z","iopub.status.idle":"2025-09-09T23:39:59.620903Z","shell.execute_reply.started":"2025-09-09T23:39:59.608310Z","shell.execute_reply":"2025-09-09T23:39:59.614435Z"}},"outputs":[{"name":"stdout","text":"TPU device: xla:0\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"import numpy as np \nfrom tqdm import tqdm, trange\nfrom sentence_transformers import SentenceTransformer\n\ntqdm.pandas()\n\nMODEL_NAME = \"all-MiniLM-L6-v2\"\nK = 5\n# Data fields IDs\n# dropped all in the list\nID_LABELS = [\"dataset_id\", \"article_id\", \"id\", \"DOI\", \"url\"]\n# data columns included in the input batch\n# dropped: issued, embedding, author, text\nTRAIN_LABELS = [ 'title', 'segments', 'extension', 'abstract', 'publisher', 'copyright', 'issued_year', 'all_authors', 'categories']\nALL_FIELDS = [\n    'article_id','text','extension','source','dataset_id','dataset_id_cited','type',\n    'id','categories','abstract','DOI','publisher','title','URL','copyright',\n    'issued_year','all_authors','y'\n]\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nLOCAL_MODEL_PATH = snap_path\n\nclass Basement(SentenceTransformer):\n    def __init__(self, initial_data: pd.DataFrame):\n        \n        super().__init__(LOCAL_MODEL_PATH, DEVICE)\n\n        self.x = self.preprocess_data(initial_data)\n        self.to(DEVICE)\n        self._target_device(TARGET_DEVICE)\n    \n        self.eval()\n        print(\"Model target device:\", model._target_device)\n        # freeze all parameters\n        for p in self.parameters():\n            p.requires_grad = False\n            \n    def _meta_conditioner(self, row):\n        combined_prompt = f\"\"\"\n        Title: {row.get('title', '')}\n        Abstract: {row.get('abstract', '')}\n        Author: {row.get('all_authors', '')}\n        Extension File Type: {row.get('extension', '')}\n        Publisher: {row.get('publisher', '')}\n        Category: {row.get('categories', '')}\n        Issued Year: {row.get('issued_year', '')}\n        Copyright: {row.get('copyright', '')}\n        \"\"\"\n        meta_embedding = self.encode([combined_prompt], convert_to_numpy=True, output_value=\"sentence_embedding\") # or token_embeddings\n        text_embedding = row.get('segments', None)\n\n        if isinstance(text_embedding, np.ndarray):\n            if text_embedding.ndim == 1:\n                text_embedding = np.expand_dims(text_embedding, axis=0)\n            return np.vstack((meta_embedding, text_embedding))\n\n        # fallback empty vector if segments missing\n        return np.empty((1, self.get_sentence_embedding_dimension()), dtype=np.float32)\n\n    def _segment_text(self, s: str, k: int = K) -> np.ndarray:\n        if not isinstance(s, str) or not s:\n            return np.zeros((k, self.get_sentence_embedding_dimension()), dtype=np.float32)\n\n        n = len(s)\n        idx = np.linspace(0, n, k + 1, dtype=int)\n        chunks = [s[idx[i]:idx[i+1]] for i in range(k)]\n        emb = self.encode(chunks, convert_to_numpy=True, show_progress_bar=False)\n        emb = np.asarray(emb, dtype=np.float32)\n        return emb.reshape(k, -1)\n\n    def preprocess_data(\n        self,\n        data: pd.DataFrame,\n        k: int = K,\n        train_labels: list[str] | None = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        - Adds 'segments' ((k, d) embeddings) from 'text'\n        - Explodes: all_authors, segments, categories\n        - Drops duplicates by TRAIN_LABELS (excluding 'segments')\n        - Fills: copyright/cats/issued_year (mode)\n        \"\"\"\n        train_labels = train_labels or TRAIN_LABELS\n\n        df = data.copy()\n\n        # 1) create 'segments'\n        df.loc[:, \"segments\"] = df[\"text\"].apply(lambda x: self._segment_text(x, k))\n\n        # 2) explode multi-valued columns\n        for col in (\"all_authors\", \"segments\", \"categories\"):\n            if col in df.columns:\n                df = df.explode(col, ignore_index=True)\n\n        # 3) drop duplicated rows (excluding 'segments')\n        subset_cols = [c for c in train_labels if c in df.columns and c != \"segments\"]\n        if subset_cols:\n            df = df.drop_duplicates(subset=subset_cols, keep=\"first\")\n\n        # 4) fills / cleaning\n        if \"issued_year\" in df.columns:\n            mode_series = df[\"issued_year\"].dropna().mode()\n            most_freq_issued = mode_series.iloc[0] if not mode_series.empty else None\n            if most_freq_issued is not None:\n                df.loc[:, \"issued_year\"] = df[\"issued_year\"].fillna(most_freq_issued)\n\n        if \"copyright\" in df.columns:\n            df.loc[:, \"copyright\"] = (\n                df[\"copyright\"].astype(\"string\").str.strip().replace({\"\": \"Unknown\"})\n            )\n\n        if \"categories\" in df.columns:\n            df.loc[:, \"categories\"] = df[\"categories\"].astype(\"string\").fillna(\"Unknown\")\n\n        df.loc[:, \"inputs\"] = df.apply(self._meta_conditioner, axis=1)\n        \n        return df\n\n    def predict(self, inputs: dict):\n        max_pred = None\n        if text := inputs.get('text', None):\n            segment = self._segment_text(text)\n            inputs.update({'segments': segment})\n            input_emb = self._meta_conditioner(inputs)\n\n            # Ensure input_emb is 2D: (1, D)\n            if input_emb.ndim == 1:\n                input_emb = input_emb.reshape(1, -1)\n\n            print(f\"test input emb shape: {input_emb.shape}\")\n\n            for grp, row in self.x.groupby(['article_id', 'dataset_id', 'source']):\n                train_sample = np.vstack(row['inputs'].values)\n\n                # Ensure train_sample is 2D as well\n                if train_sample.ndim == 1:\n                    train_sample = train_sample.reshape(1, -1)\n\n                # Now both shapes are (N, D) vs (1, D)\n                avg = self.similarity_pairwise(train_sample, input_emb).mean().item()\n                \n                prev_score = max_pred.get('score', None) if max_pred is not None else None\n                if max_pred is None or (prev_score is not None and avg > prev_score):\n                    max_pred = {\n                        'article_id': grp[0],\n                        'dataset_id': grp[1],\n                        'type': grp[2],\n                        'score': avg,\n                    }\n\n        return max_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T23:40:35.829871Z","iopub.execute_input":"2025-09-09T23:40:35.830203Z","iopub.status.idle":"2025-09-09T23:40:35.860587Z","shell.execute_reply.started":"2025-09-09T23:40:35.830176Z","shell.execute_reply":"2025-09-09T23:40:35.854348Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Load the dataset \n\nimport pandas as pd \n\ntrain_data = pd.read_parquet('/kaggle/input/make-data-count-data-preparation/train_dataset.parquet')\ntest_data = pd.read_parquet('/kaggle/input/make-data-count-data-preparation/test_dataset.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T23:37:54.475086Z","iopub.execute_input":"2025-09-09T23:37:54.475392Z","iopub.status.idle":"2025-09-09T23:37:54.749115Z","shell.execute_reply.started":"2025-09-09T23:37:54.475368Z","shell.execute_reply":"2025-09-09T23:37:54.743311Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"base = Basement(train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T23:40:42.319092Z","iopub.execute_input":"2025-09-09T23:40:42.319408Z"}},"outputs":[{"name":"stderr","text":"WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /kaggle/working/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"prediction = []\n\nfor _, row in test_data.iterrows():\n    pred = base.predict(row.to_dict())   # convert Series → dict\n    if pred:  # skip empty predictions\n        prediction.append(pred)\n\nsubmission = pd.DataFrame.from_records(prediction)\n\n# Mapping type strings → ints\npred_map_idx = {\"Primary\": 0, \"Secondary\": 1, \"Missing\": 2, \"Unknown\": 3}\nsubmission = submission.dropna(subset=['article_id', 'dataset_id', 'type'])\nsubmission['type'] = submission['type'].map(pred_map_idx)\n\n# Deduplicate and sort\nsubmission = submission.drop_duplicates(subset=[\"article_id\", \"dataset_id\", \"type\"])\nsubmission = submission.sort_values([\"article_id\", \"dataset_id\", \"type\"]).reset_index(drop=True)\n\n# Add row_index (row_id) column\nsubmission['row_index'] = submission.index\n\n# Save in required order\nsubmission[['row_index', 'article_id', 'dataset_id', 'type']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}