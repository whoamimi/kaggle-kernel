{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bcookie11/lord-of-the-submits-return-of-the-overfit?scriptVersionId=260971750\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Data Collection (Meta / Text)\n\n- Dataclasses to extract articles from `.xml` and `.pdf` for model training / tunings.\n- Learning algorithm combines greedy search algorithm and cosine similarity distance scores to recommend tuple set ","metadata":{}},{"cell_type":"code","source":"import os\n\n# Silence TF/XLA/absl chatter that spams STDERR on Kaggle\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"        # 0=all,1=INFO,2=WARNING,3=ERROR\nos.environ[\"ABSL_LOGGING_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"HF_HOME\"] = \"/kaggle/working/hf_cache\"\nos.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = \"/kaggle/working/st_cache\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nTRAIN_Y_PATH: str = \"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\"\nTRAIN_DIR_PATH:  str = \"/kaggle/input/make-data-count-finding-data-references/train\"\nMETA_PAPER_API = \"https://api.crossref.org/works/{doi}\"\nDEFAULT_SOURCE_TYPE = 'Unknown'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:00:56.923183Z","iopub.execute_input":"2025-09-10T04:00:56.923419Z","iopub.status.idle":"2025-09-10T04:00:56.931116Z","shell.execute_reply.started":"2025-09-10T04:00:56.923397Z","shell.execute_reply":"2025-09-10T04:00:56.930458Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Data Helpers and Utilities \n\nimport re\nimport io\nimport glob\nimport logging\nimport requests\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom pdfminer.high_level import extract_text\nfrom dataclasses import dataclass, field, asdict\nfrom typing import Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union, Any, Optional\n\nimport torch\nimport numpy as np\nimport urllib.parse as up\nfrom torch.utils.data import Dataset, DataLoader\nfrom sentence_transformers import SentenceTransformer\n\nlogger = logging.getLogger(\"kaggle_notebook\")\nlogger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n    \"%(asctime)s | %(levelname)-8s | %(message)s\", \"%Y-%m-%d %H:%M:%S\"\n)\nhandler.setFormatter(formatter)\n\ndef _read_file_binary(path: str) -> bytes:\n    with open(path, \"rb\") as f:\n        return f.read()\n\ndef _clean_ws(text: str) -> str:\n    text = re.sub(r\"\\r\\n?\", \"\\n\", text)\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", text)  # collapse >2 blank lines\n    return text.strip()\n\ndef _pdf_to_text(path: str) -> str:\n    \"\"\"Extract text from PDF using pdfminer.six if available, else PyPDF2 as fallback.\"\"\"\n    # Try pdfminer.six (best quality)\n    try:\n        # Note: extract_text opens file internally; pass path.\n        text = extract_text(path) or \"\"\n        return _clean_ws(text)\n    except Exception:\n        pass\n\n    # Fallback: PyPDF2\n    try:\n        import PyPDF2  # type: ignore\n        text_chunks: List[str] = []\n        with open(path, \"rb\") as f:\n            reader = PyPDF2.PdfReader(f)\n            for pg in reader.pages:\n                try:\n                    s = pg.extract_text() or \"\"\n                except Exception:\n                    s = \"\"\n                if s:\n                    text_chunks.append(s)\n        return _clean_ws(\"\\n\\n\".join(text_chunks))\n    except Exception:\n        return \"\"\n\ndef _xml_to_text(path: str) -> str:\n    \"\"\"Parse XML with lxml if available, else ElementTree. Extracts title/abstract/body-ish text.\"\"\"\n    xml_bytes = _read_file_binary(path)\n\n    # Try lxml first (best for namespaces/xpaths).\n    try:\n        from lxml import etree  # type: ignore\n        parser = etree.XMLParser(recover=True, huge_tree=True)\n        root = etree.fromstring(xml_bytes, parser=parser)\n\n        # Common scholarly XML patterns (JATS-ish)\n        texts: List[str] = []\n\n        # title\n        titles = root.xpath(\"//article-title|//title-group//article-title|//title\")\n        titles = [t.text if isinstance(t, etree._Element) else str(t) for t in titles]\n        titles = [t for t in titles if t]\n        if titles:\n            texts.append(\"# \" + titles[0].strip())\n\n        # abstract\n        abs_nodes = root.xpath(\"//abstract//p|//Abstract//p|//abstract\")\n        for n in abs_nodes:\n            s = \"\".join(n.itertext()) if hasattr(n, \"itertext\") else str(n)\n            s = s.strip()\n            if s:\n                texts.append(s)\n\n        # body\n        body_nodes = root.xpath(\"//body//p|//sec//p|//Body//p\")\n        for n in body_nodes:\n            s = \"\".join(n.itertext()) if hasattr(n, \"itertext\") else str(n)\n            s = s.strip()\n            if s:\n                texts.append(s)\n\n        # fallback: all text\n        if not texts:\n            all_text = \" \".join(root.itertext())\n            texts = [all_text]\n\n        return _clean_ws(\"\\n\\n\".join(texts))\n\n    except Exception:\n        # Fallback to stdlib ElementTree\n        import xml.etree.ElementTree as ET\n\n        try:\n            root = ET.fromstring(xml_bytes)\n        except Exception:\n            return \"\"  # unreadable\n\n        def itxt(el):\n            try:\n                return \"\".join(el.itertext())\n            except Exception:\n                return el.text or \"\"\n\n        # Attempt similar sections by tag name\n        parts: List[str] = []\n        # naive title\n        for tag in (\"article-title\", \"title\"):\n            for n in root.iter(tag):\n                s = (n.text or \"\").strip()\n                if s:\n                    parts.append(\"# \" + s)\n\n        # abstract\n        for tag in (\"abstract\",):\n            for n in root.iter(tag):\n                s = itxt(n).strip()\n                if s:\n                    parts.append(s)\n\n        # paragraphs\n        for tag in (\"p\",):\n            for n in root.iter(tag):\n                s = itxt(n).strip()\n                if s:\n                    parts.append(s)\n\n        if not parts:\n            parts = [itxt(root)]\n\n        return _clean_ws(\"\\n\\n\".join([p for p in parts if p]))\n\n@dataclass\nclass Author:\n    family: Optional[str] = None\n    given: Optional[str] = None\n    literal: Optional[str] = None\n\n@dataclass\nclass Issued:\n    date_parts: List[List[int]] = field(default_factory=list)\n\n@dataclass\nclass DoiResponse:\n    type: str\n    id: str\n    categories: List[str]\n    author: List[Author]\n    issued: Issued\n    abstract: str\n    DOI: str\n    publisher: str\n    title: str\n    URL: str\n    copyright: str\n\n    @staticmethod \n    def parse_response(data: Dict[str, Any]):\n        authors = [Author(**a) for a in data.get(\"author\", [])]\n        issued = Issued(date_parts=data.get(\"issued\", {}).get(\"date-parts\", []))\n        return DoiResponse(\n            type=data.get(\"type\", \"\"),\n            id=data.get(\"id\", \"\"),\n            categories=data.get(\"categories\", []),\n            author=authors,\n            issued=issued,\n            abstract=data.get(\"abstract\", \"\"),\n            DOI=data.get(\"DOI\", \"\"),\n            publisher=data.get(\"publisher\", \"\"),\n            title=data.get(\"title\", \"\"),\n            URL=data.get(\"URL\", \"\"),\n            copyright=data.get(\"copyright\", \"\"),\n        )\n\n\n    @staticmethod\n    def parse_crossref(data: Dict[str, Any]):\n        \"\"\"Parse Crossref API `message` response into DoiResponse.\"\"\"\n        authors = [Author(**{k: v for k, v in a.items() if k in (\"given\",\"family\",\"literal\")}) \n                   for a in data.get(\"author\", [])]\n        issued = Issued(date_parts=data.get(\"issued\", {}).get(\"date-parts\", []))\n        return DoiResponse(\n            type=data.get(\"type\", \"\"),\n            id=data.get(\"DOI\", \"\"),  # Crossref uses DOI as ID\n            categories=data.get(\"subject\", []),\n            author=authors,\n            issued=issued,\n            abstract=data.get(\"abstract\", \"\"),\n            DOI=data.get(\"DOI\", \"\"),\n            publisher=data.get(\"publisher\", \"\"),\n            title=\"\".join(data.get(\"title\", [])) if isinstance(data.get(\"title\"), list) else data.get(\"title\", \"\"),\n            URL=data.get(\"URL\", \"\"),\n            copyright=data.get(\"license\", [{}])[0].get(\"URL\", \"\"),\n        )\n        \n@dataclass\nclass Article:\n    article_id: str \n    text: str \n    extension: str \n    source: str = DEFAULT_SOURCE_TYPE\n    dataset_id: str | None = None \n    dataset_id_cited: str | None = None\n    embedding: np.ndarray | None = None\n    file_path: str | Path | None = None \n    \n    @staticmethod\n    def fetch_meta_doi(doi_url: str) -> DoiResponse | None:\n        try:\n            headers = {\"Accept\": \"application/vnd.citationstyles.csl+json\"}\n            r = requests.get(doi_url, headers=headers, timeout=30)\n            if r.status_code == 200:\n                result = r.json()\n                return DoiResponse.parse_response(result)\n                \n        except Exception as e: \n            logger.error(e)\n            return None\n            \n    @staticmethod\n    def fetch_meta_crossref(doi: str) -> DoiResponse | None:\n        \"\"\"\n        Fetch metadata for a DOI from the Crossref API and return a DoiResponse object.\n        \"\"\"\n        try:\n            api_url = f\"https://api.crossref.org/works/{up.quote(doi)}\"\n            r = requests.get(api_url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=15)\n            r.raise_for_status()\n            data = r.json().get(\"message\", {})\n            return DoiResponse.parse_crossref(data)\n        except Exception as e:\n            logger.error(\"Crossref fetch failed for %s: %s\", doi, e)\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:00:56.950712Z","iopub.execute_input":"2025-09-10T04:00:56.950902Z","iopub.status.idle":"2025-09-10T04:01:22.094102Z","shell.execute_reply.started":"2025-09-10T04:00:56.950886Z","shell.execute_reply":"2025-09-10T04:01:22.093302Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757476866.863113     151 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757476866.926779     151 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Dataset ID / URL Cleaners & Converters \nimport re \n# Test datasets that could possibly exist in the data\nsamples = [\n    {\n        \"dataset_id\": \"https://doi.org/10.1098/rspb.2016.1151\",\n        \"data\": [\"https://doi.org/10.5061/dryad.6m3n9\"],\n        \"in_text_span\": \"The data we used in this publication can be accessed from Dryad at doi:10.5061/dryad.6m3n9.\",\n        \"citation_type\": \"Primary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.1098/rspb.2018.1563\",\n        \"data\": [\"https://doi.org/10.5061/dryad.c394c12\"],\n        \"in_text_span\": \"Phenotypic data and gene sequences are available from the Dryad Digital Repository: http://dx.doi.org/10.5061/dryad.c394c12\",\n        \"citation_type\": \"Primary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.1534/genetics.119.302868\",\n        \"data\": [\"https://doi.org/10.25386/genetics.11365982\"],\n        \"in_text_span\": \"The authors state that all data necessary for confirming the conclusions presented in the article are represented fully within the article. Supplemental material available at figshare: https://doi.org/10.25386/genetics.11365982.\",\n        \"citation_type\": \"Primary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.1038/sdata.2014.33\",\n        \"data\": [\"GSE37569\", \"GSE45042\", \"GSE28166\"],\n        \"in_text_span\": \"Primary data for Agilent and Affymetrix microarray experiments are available at the NCBI Gene Expression Omnibus (GEO, http://www.ncbi.nlm.nih.gov/geo/) under the accession numbers GSE37569, GSE45042 , GSE28166\",\n        \"citation_type\": \"Primary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.12688/wellcomeopenres.15142.1\",\n        \"data\": [\"pdb 5yfp\"],\n        \"in_text_span\": \"Figure 1. Evolution and structure of the exocyst... All structural images were modelled by the authors from PDB using UCSF Chimera.\",\n        \"citation_type\": \"Secondary\",\n    },\n    {\n        \"dataset_id\": \"https://doi.org/10.3389/fimmu.2021.690817\",\n        \"data\": [\"E-MTAB-10217\", \"PRJE43395\"],\n        \"in_text_span\": \"The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found below: https://www.ebi.ac.uk/arrayexpress/, E-MTAB-10217 and https://www.ebi.ac.uk/ena, PRJE43395.\",\n        \"citation_type\": \"Secondary\",\n    },\n]\n\nACCESSION_PATTERNS = [\n    # DOI (bare \"10.\" prefix, or full http(s) doi.org link, or \"doi:10...\")\n    (re.compile(r\"^(?:https?://(?:dx\\.)?doi\\.org/|doi:)?(10\\.\\d{4,9}/\\S+)$\", re.I),\n     lambda m: f\"https://doi.org/{m.group(1)}\"),\n\n    # GEO (Gene Expression Omnibus)\n    (re.compile(r\"^GSE\\d+$\", re.I),\n     lambda m: f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={m.group(0)}\"),\n\n    # ENA run/experiment (ERR/ERS/SRR/DRR/etc.)\n    (re.compile(r\"^(ERR|ERS|SRR|SRX|SRP|DRR|DRX|DRP|ERX|ERP)\\d+$\", re.I),\n     lambda m: f\"https://www.ebi.ac.uk/ena/browser/view/{m.group(0)}\"),\n\n    # dbSNP rs IDs\n    (re.compile(r\"^rs\\d+$\", re.I),\n     lambda m: f\"https://www.ncbi.nlm.nih.gov/snp/{m.group(0)}\"),\n\n    # PDB (4-char alphanumeric IDs)\n    (re.compile(r\"^[0-9A-Za-z]{4}$\"),\n     lambda m: f\"https://www.rcsb.org/structure/{m.group(0)}\"),\n\n    # ChEMBL compounds/targets/assays\n    (re.compile(r\"^CHEMBL\\d+$\", re.I),\n     lambda m: f\"https://www.ebi.ac.uk/chembl/compound_report_card/{m.group(0)}/\"),\n\n    # DDBJ/GenBank/RefSeq nucleotide accessions (D10700, CP013147, NC_#######)\n    (re.compile(r\"^(?:[A-Z]{1,2}\\d{5,6}|NC_\\d+)$\", re.I),\n     lambda m: f\"https://www.ncbi.nlm.nih.gov/nuccore/{m.group(0)}\"),\n]\n\ndef resolve_accession(acc: str) -> Optional[str]:\n    \"\"\"Return a URL for any accession/identifier/DOI.\"\"\"\n    \n    if acc is None or (isinstance(acc, float) and pd.isna(acc)):\n        return None\n        \n    s = str(acc).strip()\n    if not s:\n        return None\n\n    # Try regex patterns\n    for pattern, builder in ACCESSION_PATTERNS:\n        m = pattern.match(s)\n        if m:\n            return builder(m)\n\n    # Special-case string prefixes\n    if s.upper().startswith(\"ENS\"):  # Ensembl\n        return f\"https://www.ensembl.org/id/{s}\"\n    if s.upper().startswith(\"IPR\"):  # InterPro\n        return f\"https://www.ebi.ac.uk/interpro/entry/{s.upper()}\"\n    if s.upper().startswith(\"CVCL_\"):  # Cellosaurus\n        return f\"https://www.cellosaurus.org/{s.upper()}\"\n    if s.upper().startswith(\"EMPIAR-\"):  # EMPIAR\n        return f\"https://www.ebi.ac.uk/empiar/{s.upper()}\"\n    if s.upper().startswith(\"HGNC:\"):  # HGNC gene IDs\n        return f\"https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/{s.upper()}\"\n    if re.match(r\"^K\\d{5}$\", s, flags=re.I):  # KEGG Orthology\n        return f\"https://www.genome.jp/dbget-bin/www_bget?ko:{s.upper()}\"\n    if s.upper().startswith(\"EPI_ISL_\"):  # GISAID\n        return f\"https://www.gisaid.org/search?query={s}\"\n\n    # If it's already an HTTP(S) URL but didn't match DOI/PDB etc., keep as-is\n    if s.lower().startswith(\"http\"):\n        return s\n\n    # Fallback\n    return s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:01:22.095455Z","iopub.execute_input":"2025-09-10T04:01:22.096045Z","iopub.status.idle":"2025-09-10T04:01:22.106835Z","shell.execute_reply.started":"2025-09-10T04:01:22.096024Z","shell.execute_reply":"2025-09-10T04:01:22.105874Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import numpy as np \nfrom tqdm import tqdm, trange\nfrom sentence_transformers import SentenceTransformer\n\ntqdm.pandas()\n\nMODEL_NAME = \"all-MiniLM-L6-v2\"\nK = 5\n# Data fields IDs\n# dropped all in the list\nID_LABELS = [\"dataset_id\", \"article_id\", \"id\", \"DOI\", \"url\"]\n# data columns included in the input batch\n# dropped: issued, embedding, author, text\nTRAIN_LABELS = [ 'title', 'segments', 'extension', 'abstract', 'publisher', 'copyright', 'issued_year', 'all_authors', 'categories']\nALL_FIELDS = [\n    'article_id','text','extension','source','dataset_id','dataset_id_cited','type',\n    'id','categories','abstract','DOI','publisher','title','URL','copyright',\n    'issued_year','all_authors','y'\n]\n\ndef prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n    \n    df = df.copy()\n    # Extracts the issued year from the issued date\n    df[\"issued_year\"] = (\n        pd.json_normalize(df[\"issued\"])\n        .explode(\"date_parts\")[\"date_parts\"]\n        .map(lambda x: int(x[0]) if isinstance(x, np.ndarray) and len(x) > 0 else None)\n    )\n    # EXTRACT AUTHORS\n\n    author_snippets = [\n        [\n            \" \".join(k for k in (j.get(\"family\"), j.get(\"given\"), j.get(\"literal\")) if k)\n            for j in i\n        ]\n        if isinstance(i, (list, np.ndarray))\n        else tuple()\n        for i in df[\"author\"].values\n    ]\n\n    assert len(author_snippets) == df.shape[0], (\n        f\"Expected {df.shape[0]} number of authors (tuples) in dataset but parsed {len(author_snippets)}. Fix query.\"\n    )\n\n    df['all_authors'] = author_snippets\n    output_fields = [i for i in ALL_FIELDS if i in df.columns]\n    \n    # CREATE TARGET LABELS\n    if 'source' in df.columns:\n        df[\"y\"] = df[\"source\"].map({\"Primary\": 0, \"Secondary\": 1, \"Missing\": 2, \"Unknown\": 3})\n        return df[~df['source'].isin(['Missing', 'Unknown'])][output_fields]\n        \n    return df[output_fields]\n    \ndef load_train_dataset():\n    \"\"\" Loads the dataset for training. The train data has labels from `../train_labels.csv`. Label fields are: (`article_id`, `dataset_id`, `type`) i.e. the predicting variables. \"\"\"\n    \n    targets = pd.read_csv(TRAIN_Y_PATH)\n    logger.info(f\"Total distinct ref type Labels: {targets['type'].unique()}\")\n    \n    for path in tqdm(Path(\"/kaggle/input\").rglob(\"*\"), desc=\"Loaded Train dataset.\"):\n        if path.parents[1].stem == 'train' and path.is_file():\n            info = {}\n            \n            ext = path.suffix\n            \n            if ext == '.pdf': \n                text = _pdf_to_text(str(path))\n            elif ext == '.xml': \n                text = _xml_to_text(str(path))\n\n            meta = targets[targets['article_id'] == path.stem]\n            \n            info['extension'] = ext \n            info['text'] = text \n            info['article_id'] = path.stem\n            info['file_path'] = str(path)\n            # info['embedding'] = model.encoder(info['text']) if text != '' else None\n            \n            if not meta.empty:\n                # only gets the first data entry meta . . .\n                metas = meta.iloc[0].to_dict()\n                info[\"source\"] = metas.get(\"type\", DEFAULT_SOURCE_TYPE)\n                info[\"dataset_id\"] = metas.get(\"dataset_id\", None)\n                info[\"dataset_id_cited\"] = resolve_accession(info[\"dataset_id\"]) if info[\"dataset_id\"] is not None else None\n            else:\n                logger.warning(\"No metadata found for %s\", path.stem)\n\n            yield Article(**info)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:01:22.107834Z","iopub.execute_input":"2025-09-10T04:01:22.108398Z","iopub.status.idle":"2025-09-10T04:01:22.351773Z","shell.execute_reply.started":"2025-09-10T04:01:22.108368Z","shell.execute_reply":"2025-09-10T04:01:22.350898Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Main Caller \n\nclass DoiData(Dataset): \n    \"\"\" Doi Dataset handler. \n    Target types:\n    'Unknown': missing from train dataset\n    'Missing': missing from data - predefined in the dataset\n    'Primary' / 'Secondary': Main Data Referencing Labels\n    \"\"\"\n    \n    def __init__(self):\n        self.data = list(load_train_dataset())\n\n        assert len(self.data) > 0, \"Empty dataset loaded to instance. Pls reconfigure path or data parser.\"\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int): \n        if idx > len(self.data) or idx < 0:\n            raise ValueError('Index out of range. Pls reconfigure processor.')\n\n        # TODO: CONVERT TO X_train, y_train outputs\n        article = self.data[idx]\n        \n        if article.dataset_id_cited:\n            if meta := Article.fetch_meta_doi(article.dataset_id_cited):\n                return {**asdict(article), **asdict(meta)}\n                \n        return asdict(article)\n\ndef setup_test_dataset() -> pd.DataFrame:\n    parquet_path = Path(\"/kaggle/working/test_dataset.parquet\")\n\n    # If parquet already exists, load and return it\n    if parquet_path.exists():\n        return pd.read_parquet(parquet_path)\n\n    # Otherwise build from scratch\n    test_data = []\n    \n    for path in tqdm(Path(\"/kaggle/input\").rglob(\"*\"), desc=\"Loaded Test dataset\"):\n        if path.is_file() and path.parents[1].stem == \"test\":\n            info = {}\n            \n            file_path = str(path)\n            ext = path.suffix\n            stem = path.stem\n            doi = stem.replace(\"_\", \"/\", 1)\n            \n            print(f\"Retrieved test: {doi}\")\n            \n            if ext == \".xml\":\n                text = _xml_to_text(file_path)\n            elif ext == \".pdf\":\n                text = _pdf_to_text(file_path)\n            else:\n                continue  # skip unsupported files\n            \n            info[\"extension\"] = ext\n            info[\"text\"] = text\n            info[\"article_id\"] = path.stem\n            info[\"file_path\"] = file_path\n            info[\"dataset_id\"] = resolve_accession(doi)\n            info[\"dataset_id_cited\"] = resolve_accession(doi)\n            \n            if meta := Article.fetch_meta_crossref(info[\"dataset_id_cited\"]):\n                info.update(**asdict(meta))\n            \n            test_data.append(info)\n    \n    test_data = pd.DataFrame.from_records(test_data)\n    test_dataset = prepare_data(test_data)\n\n    # Save for reuse\n    test_dataset.to_parquet(parquet_path)\n    # test_dataset.to_csv(parquet_path.with_suffix(\".csv\"), index=False)\n\n    return test_dataset\n    \ndef setup_train_dataset() -> pd.DataFrame:\n    parquet_path = Path(\"/kaggle/working/train_dataset.parquet\")\n\n    # If parquet already exists, load and return it\n    if parquet_path.exists():\n        return pd.read_parquet(parquet_path)\n\n    # Otherwise build from scratch\n    ds = DoiData()\n    full_data = list(ds)\n    data = pd.DataFrame.from_records(full_data)\n    train_dataset = prepare_data(data)\n\n    # Save for reuse\n    train_dataset.to_parquet(parquet_path)\n    # train_dataset.to_csv(parquet_path.with_suffix(\".csv\"), index=False)\n\n    return train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:01:22.352741Z","iopub.execute_input":"2025-09-10T04:01:22.353003Z","iopub.status.idle":"2025-09-10T04:01:22.367141Z","shell.execute_reply.started":"2025-09-10T04:01:22.352983Z","shell.execute_reply":"2025-09-10T04:01:22.366521Z"},"scrolled":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Prepare Train & Test Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\n\ntqdm.pandas()\n\nMODEL_NAME = \"all-MiniLM-L6-v2\"\nK = 5\nID_LABELS = [\"dataset_id\", \"article_id\", \"id\", \"DOI\", \"url\"]\nTRAIN_LABELS = [\n    'title', 'segments', 'extension', 'abstract', 'publisher',\n    'copyright', 'issued_year', 'all_authors', 'categories'\n]\nALL_FIELDS = [\n    'article_id','text','extension','source','dataset_id','dataset_id_cited','type',\n    'id','categories','abstract','DOI','publisher','title','URL','copyright',\n    'issued_year','all_authors','y'\n]\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass Basement(SentenceTransformer):\n    def __init__(self, initial_data: pd.DataFrame):\n        super().__init__(MODEL_NAME, device=DEVICE)\n        self.x = self.preprocess_data(initial_data)\n        self.to(DEVICE)\n        self.eval()\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def _meta_conditioner(self, row):\n        combined_prompt = f\"\"\"\n        Title: {row.get('title', '')}\n        Abstract: {row.get('abstract', '')}\n        Author: {row.get('all_authors', '')}\n        Extension File Type: {row.get('extension', '')}\n        Publisher: {row.get('publisher', '')}\n        Category: {row.get('categories', '')}\n        Issued Year: {row.get('issued_year', '')}\n        Copyright: {row.get('copyright', '')}\n        \"\"\"\n        meta_embedding = self.encode(\n            [combined_prompt],\n            convert_to_numpy=True,\n            output_value=\"sentence_embedding\",\n            show_progress_bar=False\n        )\n        text_embedding = row.get('segments', None)\n\n        if isinstance(text_embedding, np.ndarray):\n            if text_embedding.ndim == 1:\n                text_embedding = np.expand_dims(text_embedding, axis=0)\n            return np.vstack((meta_embedding, text_embedding))\n\n        return meta_embedding  # fallback to meta only\n\n    def _segment_text(self, s: str, k: int = K) -> np.ndarray:\n        if not isinstance(s, str) or not s:\n            return np.zeros((k, self.get_sentence_embedding_dimension()), dtype=np.float32)\n        n = len(s)\n        idx = np.linspace(0, n, k + 1, dtype=int)\n        chunks = [s[idx[i]:idx[i+1]] for i in range(k)]\n        emb = self.encode(chunks, convert_to_numpy=True, show_progress_bar=False)\n        return np.asarray(emb, dtype=np.float32).reshape(k, -1)\n\n    def preprocess_data(\n        self,\n        data: pd.DataFrame,\n        k: int = K,\n        train_labels: list[str] | None = None\n    ) -> pd.DataFrame:\n        train_labels = train_labels or TRAIN_LABELS\n        df = data.copy()\n\n        # 1) create 'segments'\n        df[\"segments\"] = df[\"text\"].apply(lambda x: self._segment_text(x, k))\n\n        # 2) explode multi-valued columns\n        for col in (\"all_authors\", \"segments\", \"categories\"):\n            if col in df.columns:\n                df = df.explode(col, ignore_index=True)\n\n        # 3) drop duplicated rows\n        subset_cols = [c for c in train_labels if c in df.columns and c != \"segments\"]\n        if subset_cols:\n            df = df.drop_duplicates(subset=subset_cols, keep=\"first\")\n\n        # 4) fills / cleaning\n        if \"issued_year\" in df.columns:\n            mode_series = df[\"issued_year\"].dropna().mode()\n            if not mode_series.empty:\n                df[\"issued_year\"] = df[\"issued_year\"].fillna(mode_series.iloc[0])\n\n        if \"copyright\" in df.columns:\n            df[\"copyright\"] = (\n                df[\"copyright\"].astype(\"string\").str.strip().replace({\"\": \"Unknown\"})\n            )\n\n        if \"categories\" in df.columns:\n            df[\"categories\"] = df[\"categories\"].astype(\"string\").fillna(\"Unknown\")\n\n        df[\"inputs\"] = df.apply(self._meta_conditioner, axis=1)\n        \n        return df\n\n    def predict_from_temporal_state(self, inputs: dict, k: int = 1):\n        \"\"\"\n        Greedy temporal approximation:\n        - Approximates article_id, dataset_id, and source/type by maximizing cosine similarity step by step.\n        - Similar to beam search but with greedy (top1) selection.\n        \"\"\"\n        text = inputs.get(\"text\")\n        if not text:\n            print(\"Skipping this dataset as it is probably hidden test set.\")\n            return None\n\n        # Encode query text\n        segment = self._segment_text(text)\n        inputs = dict(inputs, segments=segment)\n        input_emb = self._meta_conditioner(inputs)\n\n        if input_emb.ndim == 1:\n            input_emb = input_emb.reshape(1, -1)\n\n        q = input_emb.mean(axis=0, keepdims=True)\n        q_tensor = torch.tensor(q, device=DEVICE, dtype=torch.float32)\n\n        # --- Step 1: Greedy match for article_id ---\n        article_scores = {}\n        for aid, row in self.x.groupby(\"article_id\"):\n            valid_inputs = [\n                x for x in row[\"inputs\"].values\n                if isinstance(x, np.ndarray) and x.size > 0\n            ]\n            if not valid_inputs:\n                continue\n            train_tensor = torch.tensor(\n                np.vstack(valid_inputs), device=DEVICE, dtype=torch.float32\n            )\n            article_scores[aid] = float(cos_sim(train_tensor, q_tensor).mean())\n\n        if not article_scores:\n            return None\n        best_article = max(article_scores.items(), key=lambda x: x[1])[0]\n\n        # --- Step 2: Greedy match for dataset_id (within chosen article) ---\n        dataset_scores = {}\n        for did, row in self.x[self.x[\"article_id\"] == best_article].groupby(\"dataset_id\"):\n            valid_inputs = [\n                x for x in row[\"inputs\"].values\n                if isinstance(x, np.ndarray) and x.size > 0\n            ]\n            if not valid_inputs:\n                continue\n            train_tensor = torch.tensor(\n                np.vstack(valid_inputs), device=DEVICE, dtype=torch.float32\n            )\n            dataset_scores[did] = float(cos_sim(train_tensor, q_tensor).mean())\n\n        if not dataset_scores:\n            return {\"article_id\": best_article}\n\n        best_dataset = max(dataset_scores.items(), key=lambda x: x[1])[0]\n\n        # --- Step 3: Greedy match for source/type (within chosen dataset) ---\n        type_scores = {}\n        subset = self.x[\n            (self.x[\"article_id\"] == best_article) & (self.x[\"dataset_id\"] == best_dataset)\n        ]\n        for source, row in subset.groupby(\"source\"):\n            valid_inputs = [\n                x for x in row[\"inputs\"].values\n                if isinstance(x, np.ndarray) and x.size > 0\n            ]\n            if not valid_inputs:\n                continue\n            train_tensor = torch.tensor(\n                np.vstack(valid_inputs), device=DEVICE, dtype=torch.float32\n            )\n            type_scores[source] = float(cos_sim(train_tensor, q_tensor).mean())\n\n        if not type_scores:\n            return {\"article_id\": best_article, \"dataset_id\": best_dataset}\n\n        best_type, best_score = max(type_scores.items(), key=lambda x: x[1])\n\n        return {\n            \"article_id\": best_article,\n            \"dataset_id\": best_dataset,\n            \"type\": best_type,\n            \"score\": best_score,\n        }\n        \n    def predict(self, inputs: dict, threshold: float = 0.5):\n        # disadvantage of this method is that it is strictly greater than previous scores .. can be shit \n        \n        text = inputs.get(\"text\")\n        \n        if not text:\n            print('Skipping this dataset as it is probably hidden test set.')\n            # measure against seen \n            return None\n            \n        # Query embedding\n        segment = self._segment_text(text)\n        inputs = dict(inputs, segments=segment)\n        input_emb = self._meta_conditioner(inputs)\n        \n        if input_emb.ndim == 1:\n            input_emb = input_emb.reshape(1, -1)\n            \n        q = input_emb.mean(axis=0, keepdims=True)\n\n        q_tensor = torch.tensor(q, device=DEVICE, dtype=torch.float32)\n\n        max_pred = None\n        for grp, row in self.x.groupby(['article_id', 'dataset_id', 'source']):\n            valid_inputs = [\n                x for x in row['inputs'].values\n                if isinstance(x, np.ndarray) and x.size > 0\n            ]\n            if not valid_inputs:\n                continue\n\n            train_sample = np.vstack(valid_inputs)\n            train_tensor = torch.tensor(train_sample, device=DEVICE, dtype=torch.float32)\n\n            avg = float(cos_sim(train_tensor, q_tensor).mean())\n            prev = max_pred['score'] if max_pred else None\n            if (max_pred is None) or (avg > prev):\n                source_type = grp[2] if avg >= threshold else (\n                    \"Primary\" if grp[2] == \"Secondary\" else \"Secondary\" if grp[2] == \"Primary\" else grp[2]\n                )\n                \n                max_pred = {\n                    'article_id': grp[0],\n                    'dataset_id': grp[1],\n                    'type': source_type,\n                    'score': avg,\n                }\n        return max_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:01:22.368813Z","iopub.execute_input":"2025-09-10T04:01:22.369034Z","iopub.status.idle":"2025-09-10T04:01:22.394567Z","shell.execute_reply.started":"2025-09-10T04:01:22.369016Z","shell.execute_reply":"2025-09-10T04:01:22.393741Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#test_dataset.to_parquet('/kaggle/working/test_dataset.parquet')\n#train_dataset.to_parquet('/kaggle/working/train_dataset.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:01:22.39522Z","iopub.execute_input":"2025-09-10T04:01:22.395478Z","iopub.status.idle":"2025-09-10T04:01:22.408225Z","shell.execute_reply.started":"2025-09-10T04:01:22.395455Z","shell.execute_reply":"2025-09-10T04:01:22.407649Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_data = setup_train_dataset()\ntest_data = setup_test_dataset()\n\nbase = Basement(train_data)\nprediction = []\n\nfor _, row in test_data.iterrows():\n    pred = base.predict(row.to_dict())\n    if pred:\n        prediction.append(pred)\n\nsubmission = pd.DataFrame.from_records(prediction)\nsubmission = submission.sort_values([\"article_id\", \"dataset_id\", \"type\"]).reset_index(drop=True)\nsubmission['row_id'] = submission.index\n\nsubmission[['row_id', 'article_id', 'dataset_id', 'type']].to_csv('/kaggle/working/submission.csv', index=False)\nsubmission[['row_id', 'article_id', 'dataset_id', 'type']].to_csv('submission.csv', index=False)\nprint(f'data submitted! Saving {submission.shape[0]} submissions :3', submission.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:01:22.408932Z","iopub.execute_input":"2025-09-10T04:01:22.409198Z","iopub.status.idle":"2025-09-10T04:03:05.802877Z","shell.execute_reply.started":"2025-09-10T04:01:22.409178Z","shell.execute_reply":"2025-09-10T04:03:05.801954Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd9e77d1103441b4be9c7cb8bd03bd16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26edfe61a41c40dc9bad357a18582f2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb7ce81c18d34a5cbc100b64b6dbb095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edc24ed9a3614dd2a5078b630b0161be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fd7fc9f08184a7bb8df41a982233b66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1cc9e027446415f9c3f8a0af097ba0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cbe9169d1d144d69f79ef5f5e21b422"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"428cc4bc90804a3e93b9c7c4d0ff4b11"}},"metadata":{}},{"name":"stdout","text":"data submitted! Saving 55 submissions :3              article_id                             dataset_id     type  \\\n0  10.1002_2017jc013030         https://doi.org/10.17882/49388  Primary   \n1  10.1002_2017jc013030         https://doi.org/10.17882/49388  Primary   \n2     10.1002_ece3.4466  https://doi.org/10.5061/dryad.r6nq870  Primary   \n3     10.1002_ece3.4466  https://doi.org/10.5061/dryad.r6nq870  Primary   \n4     10.1002_ece3.5260  https://doi.org/10.5061/dryad.2f62927  Primary   \n\n      score  row_id  \n0  0.799067       0  \n1  0.800693       1  \n2  0.937313       2  \n3  0.943509       3  \n4  0.883945       4  \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:03:05.840594Z","iopub.execute_input":"2025-09-10T04:03:05.840857Z","iopub.status.idle":"2025-09-10T04:03:05.857545Z","shell.execute_reply.started":"2025-09-10T04:03:05.840835Z","shell.execute_reply":"2025-09-10T04:03:05.856786Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                      article_id  \\\n0           10.1002_2017jc013030   \n1           10.1002_2017jc013030   \n2              10.1002_ece3.4466   \n3              10.1002_ece3.4466   \n4              10.1002_ece3.5260   \n5              10.1002_ece3.5260   \n6              10.1002_ece3.6144   \n7              10.1002_ece3.6144   \n8              10.1002_ece3.6303   \n9              10.1002_ece3.6303   \n10             10.1002_ece3.9627   \n11             10.1002_ece3.9627   \n12             10.1002_ecs2.1280   \n13             10.1002_ecs2.4619   \n14             10.1002_ecs2.4619   \n15              10.1002_esp.5058   \n16              10.1002_esp.5090   \n17              10.1002_esp.5090   \n18              10.1002_mp.14424   \n19              10.1002_mp.14424   \n20            10.1002_nafm.10870   \n21          10.1021_jacs.2c06519   \n22          10.1021_jacs.2c06519   \n23          10.1021_jacs.2c06519   \n24          10.1021_jacs.2c06519   \n25          10.1021_jacs.2c06519   \n26          10.1021_jacs.2c06519   \n27          10.1021_jacs.2c06519   \n28          10.1021_jacs.2c06519   \n29           10.1038_hdy.2014.75   \n30           10.1038_hdy.2014.75   \n31            10.1039_d2cc00847e   \n32            10.1039_d2cc00847e   \n33            10.1039_d2cc00847e   \n34            10.1039_d2cc00847e   \n35            10.1039_d2cc00847e   \n36            10.1039_d2cc00847e   \n37            10.1039_d2cc00847e   \n38            10.1039_d2cc00847e   \n39            10.1039_d2cc00847e   \n40            10.1039_d2cc00847e   \n41            10.1039_d2cc00847e   \n42            10.1039_d2cc00847e   \n43            10.1039_d2cc00847e   \n44            10.1039_d2cc00847e   \n45            10.1039_d2cc00847e   \n46            10.1039_d2cc00847e   \n47            10.1039_d2cc00847e   \n48         10.1093_beheco_arz101   \n49     10.1101_2022.02.10.480011   \n50       10.1111_1365-2435.13431   \n51       10.1111_1365-2664.13168   \n52       10.1111_1365-2745.13449   \n53       10.1111_1365-2745.13449   \n54  10.1371_journal.pone.0137181   \n\n                                       dataset_id       type     score  row_id  \n0                  https://doi.org/10.17882/49388    Primary  0.799067       0  \n1                  https://doi.org/10.17882/49388    Primary  0.800693       1  \n2           https://doi.org/10.5061/dryad.r6nq870    Primary  0.937313       2  \n3           https://doi.org/10.5061/dryad.r6nq870    Primary  0.943509       3  \n4           https://doi.org/10.5061/dryad.2f62927    Primary  0.883945       4  \n5           https://doi.org/10.5061/dryad.2f62927    Primary  0.879391       5  \n6         https://doi.org/10.5061/dryad.zw3r22854    Primary  0.912225       6  \n7         https://doi.org/10.5061/dryad.zw3r22854    Primary  0.871540       7  \n8         https://doi.org/10.5061/dryad.37pvmcvgb    Primary  0.934169       8  \n9         https://doi.org/10.5061/dryad.37pvmcvgb    Primary  0.935232       9  \n10        https://doi.org/10.5061/dryad.b8gtht7h3    Primary  0.898271      10  \n11        https://doi.org/10.5061/dryad.b8gtht7h3    Primary  0.905666      11  \n12            https://doi.org/10.5061/dryad.p3fg9    Primary  0.929577      12  \n13                https://doi.org/10.25349/d9qw5x    Primary  0.899498      13  \n14                https://doi.org/10.25349/d9qw5x    Primary  0.897412      14  \n15        https://doi.org/10.5061/dryad.jh9w0vt9t    Primary  0.821912      15  \n16               https://doi.org/10.5066/p9353101  Secondary  0.747452      16  \n17               https://doi.org/10.5066/p9353101  Secondary  0.742012      17  \n18  https://doi.org/10.7937/k9/tcia.2015.pf0m9rei  Secondary  0.764617      18  \n19  https://doi.org/10.7937/k9/tcia.2015.pf0m9rei  Secondary  0.769677      19  \n20               https://doi.org/10.5066/p9gtumay    Primary  0.760283      20  \n21       https://doi.org/10.25377/sussex.21184705    Primary  0.519662      21  \n22       https://doi.org/10.25377/sussex.21184705    Primary  0.575299      22  \n23       https://doi.org/10.25377/sussex.21184705    Primary  0.538513      23  \n24       https://doi.org/10.25377/sussex.21184705    Primary  0.533922      24  \n25       https://doi.org/10.25377/sussex.21184705    Primary  0.566988      25  \n26       https://doi.org/10.25377/sussex.21184705  Secondary  0.499786      26  \n27       https://doi.org/10.25377/sussex.21184705  Secondary  0.468981      27  \n28       https://doi.org/10.25377/sussex.21184705  Secondary  0.462823      28  \n29            https://doi.org/10.5061/dryad.1p80f    Primary  0.626649      29  \n30            https://doi.org/10.5061/dryad.1p80f    Primary  0.609576      30  \n31         https://doi.org/10.5281/zenodo.6010341    Primary  0.596186      31  \n32         https://doi.org/10.5281/zenodo.6010341    Primary  0.524174      32  \n33         https://doi.org/10.5281/zenodo.6010341    Primary  0.524558      33  \n34         https://doi.org/10.5281/zenodo.6010341    Primary  0.592188      34  \n35         https://doi.org/10.5281/zenodo.6010341    Primary  0.536696      35  \n36         https://doi.org/10.5281/zenodo.6010341    Primary  0.518654      36  \n37         https://doi.org/10.5281/zenodo.6010341    Primary  0.540213      37  \n38         https://doi.org/10.5281/zenodo.6010341    Primary  0.510001      38  \n39         https://doi.org/10.5281/zenodo.6010341    Primary  0.524083      39  \n40         https://doi.org/10.5281/zenodo.6010341    Primary  0.513643      40  \n41         https://doi.org/10.5281/zenodo.6010341  Secondary  0.421222      41  \n42         https://doi.org/10.5281/zenodo.6010341  Secondary  0.494007      42  \n43         https://doi.org/10.5281/zenodo.6010341  Secondary  0.459202      43  \n44         https://doi.org/10.5281/zenodo.6010341  Secondary  0.451931      44  \n45         https://doi.org/10.5281/zenodo.6010341  Secondary  0.455336      45  \n46         https://doi.org/10.5281/zenodo.6010341  Secondary  0.491129      46  \n47         https://doi.org/10.5281/zenodo.6010341  Secondary  0.463438      47  \n48          https://doi.org/10.5061/dryad.4dj6042    Primary  0.647449      48  \n49      https://doi.org/10.15482/usda.adc/1524676  Secondary  0.416955      49  \n50          https://doi.org/10.5061/dryad.d84hg87    Primary  0.657216      50  \n51          https://doi.org/10.5061/dryad.j8cm402    Primary  0.658468      51  \n52        https://doi.org/10.5061/dryad.r7sqv9s8n    Primary  0.765154      52  \n53        https://doi.org/10.5061/dryad.r7sqv9s8n    Primary  0.768291      53  \n54            https://doi.org/10.5061/dryad.m4r46    Primary  0.663803      54  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_id</th>\n      <th>dataset_id</th>\n      <th>type</th>\n      <th>score</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10.1002_2017jc013030</td>\n      <td>https://doi.org/10.17882/49388</td>\n      <td>Primary</td>\n      <td>0.799067</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10.1002_2017jc013030</td>\n      <td>https://doi.org/10.17882/49388</td>\n      <td>Primary</td>\n      <td>0.800693</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10.1002_ece3.4466</td>\n      <td>https://doi.org/10.5061/dryad.r6nq870</td>\n      <td>Primary</td>\n      <td>0.937313</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10.1002_ece3.4466</td>\n      <td>https://doi.org/10.5061/dryad.r6nq870</td>\n      <td>Primary</td>\n      <td>0.943509</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10.1002_ece3.5260</td>\n      <td>https://doi.org/10.5061/dryad.2f62927</td>\n      <td>Primary</td>\n      <td>0.883945</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10.1002_ece3.5260</td>\n      <td>https://doi.org/10.5061/dryad.2f62927</td>\n      <td>Primary</td>\n      <td>0.879391</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10.1002_ece3.6144</td>\n      <td>https://doi.org/10.5061/dryad.zw3r22854</td>\n      <td>Primary</td>\n      <td>0.912225</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10.1002_ece3.6144</td>\n      <td>https://doi.org/10.5061/dryad.zw3r22854</td>\n      <td>Primary</td>\n      <td>0.871540</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10.1002_ece3.6303</td>\n      <td>https://doi.org/10.5061/dryad.37pvmcvgb</td>\n      <td>Primary</td>\n      <td>0.934169</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10.1002_ece3.6303</td>\n      <td>https://doi.org/10.5061/dryad.37pvmcvgb</td>\n      <td>Primary</td>\n      <td>0.935232</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10.1002_ece3.9627</td>\n      <td>https://doi.org/10.5061/dryad.b8gtht7h3</td>\n      <td>Primary</td>\n      <td>0.898271</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10.1002_ece3.9627</td>\n      <td>https://doi.org/10.5061/dryad.b8gtht7h3</td>\n      <td>Primary</td>\n      <td>0.905666</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>10.1002_ecs2.1280</td>\n      <td>https://doi.org/10.5061/dryad.p3fg9</td>\n      <td>Primary</td>\n      <td>0.929577</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>10.1002_ecs2.4619</td>\n      <td>https://doi.org/10.25349/d9qw5x</td>\n      <td>Primary</td>\n      <td>0.899498</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>10.1002_ecs2.4619</td>\n      <td>https://doi.org/10.25349/d9qw5x</td>\n      <td>Primary</td>\n      <td>0.897412</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>10.1002_esp.5058</td>\n      <td>https://doi.org/10.5061/dryad.jh9w0vt9t</td>\n      <td>Primary</td>\n      <td>0.821912</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>10.1002_esp.5090</td>\n      <td>https://doi.org/10.5066/p9353101</td>\n      <td>Secondary</td>\n      <td>0.747452</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>10.1002_esp.5090</td>\n      <td>https://doi.org/10.5066/p9353101</td>\n      <td>Secondary</td>\n      <td>0.742012</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>10.1002_mp.14424</td>\n      <td>https://doi.org/10.7937/k9/tcia.2015.pf0m9rei</td>\n      <td>Secondary</td>\n      <td>0.764617</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>10.1002_mp.14424</td>\n      <td>https://doi.org/10.7937/k9/tcia.2015.pf0m9rei</td>\n      <td>Secondary</td>\n      <td>0.769677</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>10.1002_nafm.10870</td>\n      <td>https://doi.org/10.5066/p9gtumay</td>\n      <td>Primary</td>\n      <td>0.760283</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>10.1021_jacs.2c06519</td>\n      <td>https://doi.org/10.25377/sussex.21184705</td>\n      <td>Primary</td>\n      <td>0.519662</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>10.1021_jacs.2c06519</td>\n      <td>https://doi.org/10.25377/sussex.21184705</td>\n      <td>Primary</td>\n      <td>0.575299</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>10.1021_jacs.2c06519</td>\n      <td>https://doi.org/10.25377/sussex.21184705</td>\n      <td>Primary</td>\n      <td>0.538513</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>10.1021_jacs.2c06519</td>\n      <td>https://doi.org/10.25377/sussex.21184705</td>\n      <td>Primary</td>\n      <td>0.533922</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>10.1021_jacs.2c06519</td>\n      <td>https://doi.org/10.25377/sussex.21184705</td>\n      <td>Primary</td>\n      <td>0.566988</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>10.1021_jacs.2c06519</td>\n      <td>https://doi.org/10.25377/sussex.21184705</td>\n      <td>Secondary</td>\n      <td>0.499786</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>10.1021_jacs.2c06519</td>\n      <td>https://doi.org/10.25377/sussex.21184705</td>\n      <td>Secondary</td>\n      <td>0.468981</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>10.1021_jacs.2c06519</td>\n      <td>https://doi.org/10.25377/sussex.21184705</td>\n      <td>Secondary</td>\n      <td>0.462823</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>10.1038_hdy.2014.75</td>\n      <td>https://doi.org/10.5061/dryad.1p80f</td>\n      <td>Primary</td>\n      <td>0.626649</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>10.1038_hdy.2014.75</td>\n      <td>https://doi.org/10.5061/dryad.1p80f</td>\n      <td>Primary</td>\n      <td>0.609576</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.596186</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.524174</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.524558</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.592188</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.536696</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.518654</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.540213</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.510001</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.524083</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Primary</td>\n      <td>0.513643</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Secondary</td>\n      <td>0.421222</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Secondary</td>\n      <td>0.494007</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Secondary</td>\n      <td>0.459202</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Secondary</td>\n      <td>0.451931</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Secondary</td>\n      <td>0.455336</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Secondary</td>\n      <td>0.491129</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>10.1039_d2cc00847e</td>\n      <td>https://doi.org/10.5281/zenodo.6010341</td>\n      <td>Secondary</td>\n      <td>0.463438</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>10.1093_beheco_arz101</td>\n      <td>https://doi.org/10.5061/dryad.4dj6042</td>\n      <td>Primary</td>\n      <td>0.647449</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>10.1101_2022.02.10.480011</td>\n      <td>https://doi.org/10.15482/usda.adc/1524676</td>\n      <td>Secondary</td>\n      <td>0.416955</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>10.1111_1365-2435.13431</td>\n      <td>https://doi.org/10.5061/dryad.d84hg87</td>\n      <td>Primary</td>\n      <td>0.657216</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>10.1111_1365-2664.13168</td>\n      <td>https://doi.org/10.5061/dryad.j8cm402</td>\n      <td>Primary</td>\n      <td>0.658468</td>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>10.1111_1365-2745.13449</td>\n      <td>https://doi.org/10.5061/dryad.r7sqv9s8n</td>\n      <td>Primary</td>\n      <td>0.765154</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>10.1111_1365-2745.13449</td>\n      <td>https://doi.org/10.5061/dryad.r7sqv9s8n</td>\n      <td>Primary</td>\n      <td>0.768291</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>10.1371_journal.pone.0137181</td>\n      <td>https://doi.org/10.5061/dryad.m4r46</td>\n      <td>Primary</td>\n      <td>0.663803</td>\n      <td>54</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11}]}